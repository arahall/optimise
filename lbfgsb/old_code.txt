static tuple<bool, VectorXd> subspace_minimisation(const VectorXd& x, const VectorXd& g,
												   const VectorXd& l, const VectorXd& u,
												   const VectorXd& xc, const VectorXd& c,
												   double theta, MatrixXd& w, const MatrixXd& m)
{
	size_t n = x.size();
	vector<size_t> free_vars_index;
	for (size_t i = 0; i < n; ++i)
	{
		if (xc[i] > l[i] && xc[i] < u[i])
		{
			free_vars_index.push_back(i);
		}
	}
	size_t num_free_vars = free_vars_index.size();
	if (num_free_vars == 0)
	{
		return { false, xc }; // line search not required on return
	}
	MatrixXd wz(num_free_vars, c.size());
	wz = w(free_vars_index, Eigen::all);  // Directly extracting rows using index list

	// compute the reduced gradient of mk restricted to free variables
	// rr = g + theta * (xc - x) - w *(m*c)
	VectorXd rr = g + (xc - x) * theta - w * (m * c);
	VectorXd r(num_free_vars);
	for (int i = 0; i < num_free_vars; ++i)
	{
		r[i] = rr[free_vars_index[i]];
	}
	// form intermediate variables
	
	double one_over_theta = 1.0 / theta;
	MatrixXd wz_T = wz.transpose();
	VectorXd v = m * wz_T * r;
	MatrixXd wz_T_wz = wz_T * wz;
	MatrixXd big_n = MatrixXd::Identity(wz_T_wz.rows(), wz_T_wz.cols());
	big_n.noalias() -= m * wz_T_wz * one_over_theta;  // Avoid aliasing
	v = big_n.colPivHouseholderQr().solve(v); 
	VectorXd du = -one_over_theta * r - one_over_theta * one_over_theta * wz * v;

	// find alpha star
	double alpha_star = find_alpha(l, u, xc, du, free_vars_index);

	// compute the subspace minimisation
	VectorXd xbar(xc);
	for (size_t i = 0; i < num_free_vars; ++i)
	{
		size_t idx = free_vars_index[i];
		xbar[idx] += alpha_star * du[i];
	}
	return { true, xbar };
}
bool LBFGSB::optimize(function<double(const VectorXd&)> func,
					  function<VectorXd(const VectorXd&)> gradient,
					  VectorXd& x, const VectorXd& lb, const VectorXd& ub,
					  int max_history, int max_iter, int ln_srch_maxiter, 
					  double tol, double c1, double c2, double alpha_max,
					  double eps_factor, bool debug)
{
	// func - the function to be minimised
	// gradient - the gradient of the function to be minimosed
	// x - the solution 
	// max_history - number of corrections used in the limited memeory matrix
	// max_history < 3 not recommended, large m not recommend
	// 3 <= m < 20 is the recommended range for me
	//
	if (debug)
	{
		std::cout << "max_history: " << max_history << " max_iter: " << max_iter << " ln_srch_maxiter: " << ln_srch_maxiter <<
			" tol: " << tol << " c1: " << c1 << " c2: " << c2 << " alpha_max: " << alpha_max << endl;
	}
	size_t n = x.size(); // the problem dimension
	double tol_f = std::numeric_limits<double>::epsilon() * eps_factor;
	// check that the bounds are well specified
	for (unsigned i = 0; i < n; ++i)
	{
		if (lb[i] >= ub[i])
		{
			throw runtime_error("LBFGSB::optimise - lower bound must be less than upper boound");
		}
	}
	MatrixXd w = MatrixXd::Zero(n, 1), m = MatrixXd::Zero(1, 1);
	MatrixXd y_history, s_history;

	double f = func(x);
	VectorXd g = gradient(x);
	if (g.size() != n)
	{
		throw runtime_error("LBFGSB::optimise - length of gradient must be the same as the problem dimension");
	}
	double theta = 1.0;
	for (int iter = 0; iter < max_iter; ++iter)
	{
		double opt = get_optimality(x, g, lb, ub);
		if (debug)
		{
			cout << "optimality = " << opt << " func = " << f << "\n";
			for (const auto& x_ : x)
			{
				cout << x_ << " ";
			}
			cout << "\n";
		}
		if (opt < tol)
		{
			if (debug)
			{
				cout << "converged in " << iter << " iterations\n";
			}
			return true;
		}
		VectorXd x_old(x);
		VectorXd g_old(g);

		// compute new search directon
		tuple<VectorXd, VectorXd> cp = get_cauchy_point(x, g, lb, ub, theta, w, m);
		VectorXd xc = get<0>(cp);
		VectorXd c = get<1>(cp);
		tuple<bool, VectorXd> sm = subspace_minimisation(x, g, lb, ub, xc, c, theta, w, m);
		bool flag = get<0>(sm);
		VectorXd xbar = get<1>(sm);
		VectorXd dx = xbar - x;
		double alpha = flag ? strong_wolfe(func, gradient, x, f, g, dx, ln_srch_maxiter, c1, c2, alpha_max) : 1.0;
		x += alpha * dx;
		double f_new = func(x);
		if (debug)
		{
			std::cout << "f_new: " << f_new << " f: " << f << "\n";
		}
		double f_tol_check = fabs(f_new - f) / max(max(abs(f), 1.0), abs(f_new));
		if (f_tol_check <= tol_f)
		{
			if (debug)
			{
				cout << "converged in " << iter << "iterations due to function tolerance: " <<
					f_tol_check << "tolf: " << tol_f << "\n";
				return true;
			}
		}
		f = f_new;
		g = gradient(x);
		dx = x - x_old;
		VectorXd dg = g - g_old;
		double curv = dx.dot(dg);
		if (curv >= numeric_limits<double>::epsilon())
		{
			if (y_history.cols() == max_history)
			{
				remove_end_column(y_history);
				remove_end_column(s_history);
			}
			add_end_column(y_history, dg);
			add_end_column(s_history, dx);
			theta = dg.dot(dg) / dg.dot(dx);
			w = MatrixXd(y_history.rows(), y_history.cols() + s_history.cols());
			w << y_history, theta* s_history;
			m = hessian(s_history, y_history, theta);
		}
		if (debug && curv < numeric_limits<double>::epsilon())
		{
			cout << "optimise - negative curvature detected. Hessian update skipped\n";
		}
	}
	return false;
}
